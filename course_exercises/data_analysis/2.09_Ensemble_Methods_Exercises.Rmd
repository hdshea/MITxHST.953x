---
title: "HST.953x Workshop 2.09: Ensemble Methods Exercises"
author: "H. David Shea"
date: "`r format(Sys.time(), '%d %b %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    fig.align: center
    fig_caption: yes
    fig_height: 5
    fig_width: 9
    highlight: tango
    theme: united
    #toc: yes
---

(Note:  Updated and modified from ["R for Statistical Learning"](https://daviddalpiaz.github.io/r4sl/).)

```{r setup, include=FALSE}
library(tidyverse)
library(rpart)
library(rpart.plot)
#library(randomForest)
library(party)
library(gbm)
library(caret)
library(MASS)
#library(ISLR)

knitr::opts_chunk$set(
    connection = "mimic3", # automatically uses this connection in sql chunks 
    comment = "#>", 
    collapse = TRUE, 
    message = FALSE,
    fig.width = 8,
    fig.asp = ((1 + sqrt(5)) / 2) - 1, # the golden ratio - technically, the b proportion of a+b when a is 1
    out.width = "70%",
    fig.align = "center"
)
```

## Description

This section contains examples of ensemble - bagging and boosting - methods.

## Dataset

We will be usingthe `Boston` dataset from the `MASS` package for these examples.  The dataset contains housing value data for the suburbs of Boston.

* **crim** - per capita crime rate by town.
* **zn** - proportion of residential land zoned for lots over 25,000 sq.ft.
* **indus** - proportion of non-retail business acres per town.
* **chas** - Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
* **nox** - nitrogen oxides concentration (parts per 10 million).
* **rm** - average number of rooms per dwelling.
* **age** - proportion of owner-occupied units built prior to 1940.
* **dis** - weighted mean of distances to five Boston employment centres.
* **rad** - index of accessibility to radial highways.
* **tax** - full-value property-tax rate per \$10,000.
* **ptratio** - pupil-teacher ratio by town.
* **black** - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
* **lstat** - lower status of the population (percent).
* **medv** - median value of owner-occupied homes in \$1000s.



```{r base_data, message=FALSE, warning=FALSE}
base_dir <- here::here("")

calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

str(Boston)
```

## Test-Train Split the Data

```{r test_train, message=FALSE, warning=FALSE}
set.seed(18)
boston_idx <- sample(1:nrow(Boston), nrow(Boston) / 2)
boston_trn <- Boston[boston_idx,]
boston_tst <- Boston[-boston_idx,]
```

## Initial Tree Model of Median Value versus all other Predictors

```{r initial_tree, message=FALSE, warning=FALSE}
boston_tree <- rpart(medv ~ ., data = boston_trn)

boston_tree_tst_pred <- predict(boston_tree, newdata = boston_tst)
plot(boston_tree_tst_pred, boston_tst$medv, 
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Single Tree, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)

# RMSE
(tree_tst_rmse <- calc_rmse(boston_tree_tst_pred, boston_tst$medv))
```

## Initial Linear Model of Median Value versus all other Predictors

```{r initial_lm, message=FALSE, warning=FALSE}
boston_lm <- lm(medv ~ ., data = boston_trn)

boston_lm_tst_pred <- predict(boston_lm, newdata = boston_tst)
plot(boston_lm_tst_pred, boston_tst$medv,
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Linear Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)

# RMSE
(lm_tst_rmse <- calc_rmse(boston_lm_tst_pred, boston_tst$medv))
```

## Bagging

(The reference book used `randomForest` but this is not yet available for M1 machines. I am trying the `cforest` function from the `party` package as a replacement.)

Bagging is actually a special case of a random forest where `mtry` is equal to the number of predictors - 13 in this case.

```{r initial_bagging, message=FALSE, warning=FALSE}
boston_bag <- cforest(medv ~ ., data = boston_trn, controls = cforest_unbiased(mtry = 13, ntree = 500))

boston_bag

boston_bag_tst_pred <- predict(boston_bag, newdata = boston_tst)
plot(boston_bag_tst_pred,boston_tst$medv,
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Bagged Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)

(bag_tst_rmse <- calc_rmse(boston_bag_tst_pred, boston_tst$medv))
```

Here we see two interesting results. First, the predicted versus actual plot no longer has a small number of predicted values. Second, our test error has dropped dramatically - note, not as dramatically as the original example did using `randomForest`.

## Random Forest 

We now try a random forest.  For regression, the suggestion for `mtry` is number of predictors divided by 3 - 4 in this case.

```{r initial_random, message=FALSE, warning=FALSE}
boston_forest <- cforest(medv ~ ., data = boston_trn, controls = cforest_unbiased(mtry = 4, ntree = 500))

boston_forest

boston_forest_tst_pred <- predict(boston_forest, newdata = boston_tst)
plot(boston_forest_tst_pred, boston_tst$medv,
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Random Forest, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)

(forest_tst_rmse <- calc_rmse(boston_forest_tst_pred, boston_tst$medv))
```

## Boosting

```{r initial_boost, message=FALSE, warning=FALSE}
booston_boost <- gbm(medv ~ ., data = boston_trn, distribution = "gaussian", 
                    n.trees = 5000, interaction.depth = 4, shrinkage = 0.01)

booston_boost

tibble::as_tibble(summary(booston_boost))

par(mfrow = c(1, 3))
plot(booston_boost, i = "rm", col = "dodgerblue", lwd = 2)

plot(booston_boost, i = "lstat", col = "dodgerblue", lwd = 2)

plot(booston_boost, i = "dis", col = "dodgerblue", lwd = 2)

boston_boost_tst_pred = predict(booston_boost, newdata = boston_tst, n.trees = 5000)
(boost_tst_rmse <- calc_rmse(boston_boost_tst_pred, boston_tst$medv))
```

```{r initial_boost_plot, message=FALSE, warning=FALSE}
plot(boston_boost_tst_pred, boston_tst$medv,
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual", 
     main = "Predicted vs Actual: Boosted Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)
```

## Results

```{r summary, message=FALSE, warning=FALSE, echo=FALSE}
boston_rmse <- tibble(
  Model = c("Single Tree", "Linear Model", "Bagging",  "Random Forest",  "Boosting"),
  TestError = c(tree_tst_rmse, lm_tst_rmse, bag_tst_rmse, forest_tst_rmse, boost_tst_rmse)
  )

knitr::kable(boston_rmse)
```

## Linear Model of Median Value versus two most significant predictors from boosting (`lstat` and `rm`)

```{r initial_lm2, message=FALSE, warning=FALSE}
boston_lm2 <- lm(medv ~ lstat + rm, data = boston_trn)

boston_lm2_tst_pred <- predict(boston_lm2, newdata = boston_tst)
plot(boston_lm2_tst_pred, boston_tst$medv,
     xlim = c(0,50), ylim = c(0,50),
     xlab = "Predicted", ylab = "Actual",
     main = "Predicted vs Actual: Linear Model, Test Data",
     col = "dodgerblue", pch = 20)
grid()
abline(0, 1, col = "darkorange", lwd = 2)

# RMSE
(lm2_tst_rmse <- calc_rmse(boston_lm_tst_pred, boston_tst$medv))
```
